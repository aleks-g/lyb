# SPDX-FileCopyrightText: 2024 Aleksander Grochowicz & Koen van Greevenbroek
#
# SPDX-License-Identifier: GPL-3.0-or-later

import hashlib
import math
import os
import re
import yaml
import shutil
import snakemake
import subprocess
from pathlib import Path

from scripts.workflow_utilities import (
    build_intersection_scenarios,
    hash_config,
    parse_net_spec,
    parse_year_wildcard,
    validate_config,
)

from snakemake.utils import min_version

# A recent version of snakemake is required for the module prefix
# keyword support.
min_version("7.0.1")

# # If we are running in cluster mode, set the shadow prefix to the $SCRATCH
# # directory. This will copy rule input files to the local scratch directory, and
# # output files back. This is to prevent too much load on the shared filesystem.
# if workflow.run_local and os.environ.get("SCRATCH") is not None:
#     workflow.shadow_prefix = os.environ.get("SCRATCH")

# if workflow.shadow_prefix:
#     print(f"Using shadow prefix {workflow.shadow_prefix}")


# First, validate the configuration files to make sure we do not make
# any silly mistakes.
validate_config(config)


# Each snakemake run is defined by a named configuration file, which
# is given as a command-line argument. This name is constant for the
# whole snakemake run.
run_name = config["name"]
results_dir = "results/" + run_name
networks_dir = "networks/" + run_name

# Create cache folder with a hash so that we could reuse previous runs
# with the same configuration and keep track of the configuration
# files. Does not hash the values iterations, conv_epsilon,
# conv_iterations of the "near_opt_approx" config.
hash_run = hash_config(config)
cache_dir = Path(workflow.current_basedir).parent / "cache" / run_name / hash_run
debug_dir = Path(workflow.current_basedir).parent / "debug" / run_name / hash_run
config_file = cache_dir / f"config-{run_name}.yaml"
cache_dir.mkdir(parents=True, exist_ok=True)
with open(config_file, "w") as f:
    yaml.dump(config, f)

print(
    f"If running compute_near_opt, using cache directory 'cache/{run_name}/{hash_run}'"
)

# Extract the custom pypsa-lyb config section from the top-level
# workflow config, and update the "run" section.
custom_pypsa_lyb_config = config["pypsa-longyearbyen"]
#custom_pypsa_lyb_config["run"]["name"] = run_name

# Read the default config, and update it using our custom config.
with open(workflow.source_path("modules/pypsa-longyearbyen/config.yml"), "r") as f:
    pypsa_lyb_config = yaml.safe_load(f)
snakemake.utils.update_config(pypsa_lyb_config, custom_pypsa_lyb_config)


# TODO: Update this to PyPSA-LYB.
# Set the number of threads to use for network optimisations.
# Note: This may need to be changed if a different solver than Gurobi is used.
grb_threads = config["pypsa-longyearbyen"]["solving"]["threads"]
parallel_threads = grb_threads * config["near_opt_approx"]["num_parallel_solvers"]

# TODO: Update this to PyPSA-LYB.
# Use the following to expand scenario configs
spec = "{year}_{simpl}_{opts}_{planning_horizon}"


# TODO: Can we remove this?
# Some results depend on collections on model scenarios. Since the
# snakemake rule DAG is based on filenames, we need to encode these
# entire collections of scenarios in filenames. However, since a list
# of scenarios (each containing wildcards relating to resolution,
# technologies, etc.) would make for very long filenames, we instead
# hash collections of scenarios and use the hash in filenames instead.
# The `scenario_collections` dictionary records the collection of
# scenarios corresponding to each hash value. (We keep a dictionary
# with multiple hash values since different rules, which may be run
# within the same workflow, may work with different scenario
# specifications.)
scenario_collections = {}
if "scenario" in config:
    near_opt_scenarios = expand(spec + "_e{eps}", **config["scenario"])
    near_opt_hash = hashlib.md5("".join(near_opt_scenarios).encode()).hexdigest()[:8]
    scenario_collections[near_opt_hash] = near_opt_scenarios


# Define the pypsa-lyb module
module pypsalyb:
    snakefile:
        "modules/pypsa-longyearbyen/Snakefile"
    config:
        pypsa_lyb_config
    prefix:
        "workflow/modules/pypsa-longyearbyen"


use rule * from pypsalyb as pypsalyb_*

# TODO: Update this to PyPSA-LYB.
wildcard_constraints:
    # wildcards from pypsa-eur(-sec):
    simpl="[a-za-z0-9]*|all",
    opts=r"[-+a-zA-Z0-9\.]*",
    planning_horizons="[0-9]{4}|",
    # The {name} wildcard is used as an option prefix giving a
    # descriptive name. As a wildcard, it must end with an underscore
    # to separate it cleanly from the rest of filenames.
    name="([-a-z0-9]*[a-z]+[-a-z0-9]*_)?",
    # The {year} wildcard represents a set of years and consists of a
    # number of single years or ranges (of the form 2000-2020) all
    # separated by `+`s. Example: "1980+1990-2000+2020".
    year=r"([0-9]+)(-[0-9]+)?(\+([0-9]+)(-[0-9]+)?)*",
    # MAA-related wildcards:
    # {eps} for 'epsilon' is a floating point number, optionally
    # followed by the suffix "uni".
    eps=r"[0-9\.]+(uni)?",
    # {epsf} for 'epsilon float' is just a floating point number, for
    # when we want to exclude the possibility of a "uni" bound in the
    # {eps} wildcard. Example: "0.15".
    epsf=r"[0-9\.]+",
    # {scenario_hash} is a hex digest of an md5 hash of a collection
    # of scenarios, used to identify said scenarios.
    scenario_hash="[0-9a-f]{8}",


localrules:
    build_all_networks,
    compute_all_optimum,
    compute_all_near_opt,
    calc_obj_bound,

localrules:
    pypsalyb_load_data,
    pypsalyb_build_network,
    pypsalyb_solve_network,


if "scenario" in config:
    rule build_all_networks:
        input:
            expand(
                os.path.join(
                    networks_dir,
                    spec + ".nc",
                ),
                **config["scenario"]
            ),

    rule compute_all_optimum:
        input:
            expand(
                os.path.join(results_dir, "optimum/" + spec + ".nc"),
                **config["scenario"]
            ),

    rule compute_all_near_opt:
        input:
            expand(
                os.path.join(
                    results_dir,
                    "near_opt/" + spec + "_e{eps}_" + near_opt_hash + ".csv",
                ),
                **config["scenario"]
            ),
    rule sample_all:
        input:
            expand(
                os.path.join(
                    results_dir,
                    "samples/" + spec + "_e{eps}_" + near_opt_hash,
                    "flag.done",
                ),
                **config["scenario"]
            ),


rule build_network:
    input:
        "workflow/modules/pypsa-longyearbyen/networks/"
        + run_name
        + "/base.nc",
    output:
        os.path.join(networks_dir, "{spec}.nc"),
    shell:
        "cp {input} {output}"



# TODO: Update this to PyPSA-LYB.
rule compute_optimum:
    input:
        network=os.path.join(networks_dir, "{spec}.nc"),
    output:
        # Example: "optimum/1980-2020_181_90m_lcopt_Co2L-3H.nc"
        #                   <------------spec------------->
        optimum=os.path.join(results_dir, "optimum/{spec}.nc"),
        obj=os.path.join(results_dir, "optimum/{spec}.obj"),
        optimal_point=os.path.join(results_dir, "optimum/{spec}.csv"),
    log:
        os.path.join("logs", run_name, "optimum/{spec}.log"),
    benchmark:
        os.path.join("benchmarks", run_name, "optimum/{spec}.tsv")
    conda:
        "envs/maa.fixed.yaml"
    resources:
        mem_mb=10000,
        runtime=100,
    # retries: 3
    threads: grb_threads
    shadow:
        "copy-minimal"
    script:
        "scripts/compute_optimum.py"


def calc_obj_bound_input(w):
    scenarios = scenario_collections[w.scenarios_hash]
    return [
        os.path.join(
            results_dir,
            f"optimum/{scenario}.obj",
        )
        for scenario in scenarios
    ]

# Local rule:
rule calc_obj_bound:
    input:
        calc_obj_bound_input,
    output:
        # Example: "obj_bound/e0.05uni_ad80987f"
        #                      epsf    <-hash->
        # Note the use of "epsf", restricted to a floating point
        # number. The "uni" part is _fixed_ in the output filename for
        # this rule, so the {eps} wildcard must end on "uni".
        os.path.join(results_dir, "obj_bound/e{epsf}uni_{scenarios_hash}"),
    resources:
        mem_mb=10,
    script:
        "scripts/calc_obj_bound.py"


def near_opt_memory(wildcards):
    return config["near_opt_approx"].get(
        "num_parallel_solvers", 1
    ) * 10000


def mga_runtime(wildcards):
    """Upper bound on MGA runtime in minutes."""
    num_parallel = int(config["near_opt_approx"].get("num_parallel_solvers", 1))
    num_opts = 2 * len(config["projection"])
    # Give a generous amount of slack in case of single optimisations
    # that take a long time.
    return 4 * math.ceil(num_opts / num_parallel) * 100

#TODO: Update to PyPSA_LYB
rule mga:
    input:
        network=os.path.join(results_dir, "optimum/{spec}.nc"),
        optimum=os.path.join(results_dir, "optimum/{spec}.csv"),
        obj_bound=os.path.join(results_dir, "obj_bound/e{eps}_{scenarios_hash}"),
    output:
        # Example: "mga/1980-2020_Co2L-3H_e0.2uni1980-2020.csv"
        #               <-year-> <-spec--><--------eps------->
        mga_space=os.path.join(
            results_dir, "mga/{spec}_e{eps}_{scenarios_hash}.csv"
        ),
    params:
        iterations=os.path.join(
            debug_dir,
            "mga/{spec}_e{eps}_{scenarios_hash}",
        ),
    log:
        os.path.join("logs", run_name, "mga/{spec}_e{eps}_{scenarios_hash}.log"),
    conda:
        "envs/maa.fixed.yaml"
    resources:
        mem_mb=near_opt_memory, 
        runtime=100,
        disk_mb=2 * len(config["projection"]) * 2000,
    # retries: 3
    # Make snakemake prioritise finishing these runs before compute_near_opt
    priority: 10
    threads: parallel_threads
    shadow:
        "copy-minimal"
    script:
        "scripts/mga.py"

# TODO: Consider updating this for PyPSA-LYB.
def near_opt_runtime(wildcards):
    """Upper bound on near-opt approx runtime in minutes."""
    num_parallel = int(config["near_opt_approx"].get("num_parallel_solvers", 1))
    num_opts = int(config["near_opt_approx"]["iterations"])
    # Give some slack (factor of 1.2) just in case.
    return 1.2 * (num_opts / num_parallel) * 5

# TODO: Update this to PyPSA-LYB.
rule compute_near_opt:
    input:
        network=os.path.join(results_dir, "optimum/{spec}.nc"),
        mga_space=os.path.join(
            results_dir, "mga/{spec}_e{eps}_{scenarios_hash}.csv"
        ),
        obj_bound=os.path.join(results_dir, "obj_bound/e{eps}_{scenarios_hash}"),
    params:
        iterations=os.path.join(
            debug_dir,
            "near_opt/{spec}_e{eps}_{scenarios_hash}",
        ),
        cache=os.path.join(
            cache_dir,
            "near_opt/{spec}_e{eps}_{scenarios_hash}",
        ),
    output:
        # Example: "near_opt/1980-2020_Co2L-3H_e0.2uni1980-2020.csv"
        #                    <-year--> <-spec-><-------eps----->
        near_opt=os.path.join(
            results_dir, "near_opt/{spec}_e{eps}_{scenarios_hash}.csv"
        ),
    log:
        os.path.join(
            "logs", run_name, "near_opt/{spec}_e{eps}_{scenarios_hash}.log"
        ),
    benchmark:
        os.path.join(
            "benchmarks", run_name, "near_opt/{spec}_e{eps}_{scenarios_hash}.tsv"
        )
    conda:
        "envs/maa.fixed.yaml"
    resources:
        mem_mb=near_opt_memory,
        disk_mb=config["near_opt_approx"]["iterations"] * 2000,
        runtime=lambda wildcards: near_opt_runtime(wildcards),
    # retries: 3
    threads: parallel_threads
    shadow:
        "copy-minimal"
    script:
        "scripts/compute_near_opt.py"


def sample_resources(wildcards):
    num_samples = 0
    for level_set in config["sample"]["level_sets"]:
        num_samples += level_set["samples"] * int((level_set["max_level"] - level_set["min_level"]) / level_set["step"])
    runtime = 100 * (num_samples / config["sample"]["num_parallel"])
    return runtime

def sample_inputs(wildcards):
    level_sets = config["sample"]["level_sets"]
    slacks = []
    for level_set in level_sets:
        # multiple by 100 for numerical stability
        slacks.extend(range(int(100 * level_set["min_level"]),
                            int(100 * level_set["max_level"]),
                            int(100 * level_set["step"])))
    slacks = [s/100 for s in slacks]
    return [os.path.join(
            results_dir, f"near_opt/{wildcards.spec}_e{slack}uni_{wildcards.scenarios_hash}.csv"
        ) for slack in slacks]


rule sample_near_opt_space:
    input:
        network=os.path.join(results_dir, "optimum/{spec}.nc"),
        near_opt=sample_inputs,
    params:
        num_parallel=config["sample"]["num_parallel"],
    output:
        # flag = os.path.join(
        #     results_dir,
        #     "samples",
        #     "{spec}_e{eps}_{scenarios_hash}",
        #     "flag.done",
        # ),
        samples = os.path.join(
            results_dir,
            "samples",
            "{spec}_e{eps}_{scenarios_hash}",
            "samples.csv",
        ),
        metrics = os.path.join(
            results_dir,
            "samples",
            "{spec}_e{eps}_{scenarios_hash}",
            "metrics.csv",
        ),
    log:
        os.path.join(
            "logs", run_name, "samples/{spec}_e{eps}_{scenarios_hash}.log"
        ),
    conda:
        "envs/maa.fixed.yaml"
    resources:
        mem_mb=10000 * config["sample"]["num_parallel"],
        runtime=sample_resources,
        disk_mb=2000 * config["sample"]["num_parallel"],
    # retries: 3
    threads: config["sample"]["num_parallel"] * grb_threads
    shadow:
        "copy-minimal"
    script:
        "scripts/sample_near-opt_space.py"